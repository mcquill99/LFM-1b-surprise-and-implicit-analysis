{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "from scipy import stats\n",
    "from scipy import sparse\n",
    "import scipy\n",
    "from collections import defaultdict\n",
    "import implicit\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from surprise import Reader, Dataset\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import SVDpp\n",
    "import rectorch\n",
    "from rectorch.models.baseline import SLIM, Random, Popularity\n",
    "from rectorch.models.mf import EASE\n",
    "from rectorch.data import DataProcessing\n",
    "from rectorch.samplers import ArrayDummySampler, SparseDummySampler\n",
    "from rectorch.evaluation import evaluate\n",
    "from rectorch.utils import collect_results, prepare_for_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data\n",
    "item_threshold = 1 # used to filter out user/artist pairs that have been \n",
    "                   #listened to less than the threshold number of times\n",
    "popular_artist_fraction = 0.2 # top cutoff for what we consider popular artists, in this case the top 20%\n",
    "\n",
    "user_events_file = 'data/user_events.txt'\n",
    "low_user_file = 'data/low_main_users.txt'\n",
    "medium_user_file = 'data/medium_main_users.txt'\n",
    "high_user_file = 'data/high_main_users.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of user events: 28718087\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>artist</th>\n",
       "      <th>album</th>\n",
       "      <th>track</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31435741</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1385212958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31435741</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1385212642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31435741</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1385212325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31435741</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1385209508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31435741</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1385209191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user  artist  album  track   timestamp\n",
       "0  31435741       2      4      4  1385212958\n",
       "1  31435741       2      4      4  1385212642\n",
       "2  31435741       2      4      4  1385212325\n",
       "3  31435741       2      4      4  1385209508\n",
       "4  31435741       2      4      4  1385209191"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in user events file\n",
    "cols = ['user', 'artist', 'album', 'track', 'timestamp']\n",
    "df_events = pd.read_csv(user_events_file, sep='\\t', names=cols)\n",
    "print('No. of user events: ' + str(len(df_events)))\n",
    "df_events.head() # check it is all read in properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Artist Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. user-artist pairs: 1755361\n"
     ]
    }
   ],
   "source": [
    "# create unique user-artist matrix\n",
    "df_events = df_events.groupby(['user', 'artist']).size().reset_index(name='listens')\n",
    "print('No. user-artist pairs: ' + str(len(df_events)))\n",
    "# each row contains a unique user-artist pair, along with how many times the\n",
    "# user has listened to the artist\n",
    "#df_events.sort_values(by=['user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>artist</th>\n",
       "      <th>listens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1021445</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021445</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1021445</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1021445</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021445</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>1021445</td>\n",
       "      <td>1864220</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>1021445</td>\n",
       "      <td>1864221</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>1021445</td>\n",
       "      <td>1864222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>1045479</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>1045479</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user   artist  listens\n",
       "0     1021445       12       43\n",
       "1     1021445       16        1\n",
       "2     1021445       28        7\n",
       "3     1021445       29        1\n",
       "4     1021445       46        1\n",
       "...       ...      ...      ...\n",
       "1395  1021445  1864220        3\n",
       "1396  1021445  1864221        4\n",
       "1397  1021445  1864222        1\n",
       "1398  1045479        3        9\n",
       "1399  1045479       11        1\n",
       "\n",
       "[1400 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_events.head(1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. filtered user-artist pairs: 1755361\n",
      "No. unique artists: 352805\n"
     ]
    }
   ],
   "source": [
    "# filters out artist/user pairs who havent been listened two more than\n",
    "# item_threshold amount of times to reduce potential load\n",
    "# kept to 1 currently, so we dont filter out any data\n",
    "df_events = df_events[df_events['listens'] >= item_threshold] \n",
    "\n",
    "# With 1, we see no difference between user-artist pairs here\n",
    "print('No. filtered user-artist pairs: ' + str(len(df_events))) \n",
    "\n",
    "# here, we see the number of unique artists in our matrix\n",
    "print('No. unique artists: ' + str(len(df_events['artist'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many artists have users listened to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean artists of all users: 585.1203333333333\n",
      "Min artists of all users: 18\n",
      "Max artists of all users: 4011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41888522    4011\n",
       "4393555     3700\n",
       "40029632    3678\n",
       "26874346    3544\n",
       "29736410    3529\n",
       "Name: user, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get matrix where each row is a user-id and how many artists they've \n",
    "#listened to\n",
    "user_dist = df_events['user'].value_counts() \n",
    "\n",
    "# counts how many unique users there are. prints out user id & a count of how \n",
    "# many rows they're included in, which effectively shows how many artists \n",
    "# they listen to\n",
    "num_users = len(user_dist)\n",
    "print('Mean artists of all users: ' + str(user_dist.mean()))\n",
    "print('Min artists of all users: ' + str(user_dist.min()))\n",
    "print('Max artists of all users: ' + str(user_dist.max()))\n",
    "\n",
    "user_dist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many users listen to an artist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. artists: 352805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 135        1389\n",
       "1602       1359\n",
       "46         1325\n",
       "320        1297\n",
       "27         1290\n",
       "           ... \n",
       "3124286       1\n",
       "1029181       1\n",
       "1023032       1\n",
       "1008679       1\n",
       "3087545       1\n",
       "Name: artist, Length: 352805, dtype: int64>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get artist distribution\n",
    "# same as previous but with artists, shows artist-id and how many times they\n",
    "# were listened to buy unique users\n",
    "artist_dist = df_events['artist'].value_counts()\n",
    "num_artists = len(artist_dist)\n",
    "print('No. artists: ' + str(num_artists))\n",
    "df_events['artist'].value_counts().head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. top artists: 70561\n"
     ]
    }
   ],
   "source": [
    "# get number of  popular artists\n",
    "num_top_artists = int(popular_artist_fraction * num_artists)\n",
    "\n",
    "# getting the top top_fraction (0.2) percent of artists, so finding how many\n",
    "# artists make up 20% of total artists, and then only using the artists those\n",
    "#number of the most popular aritsts\n",
    "top_artist_dist = artist_dist[:num_top_artists]\n",
    "print('No. top artists: ' + str(len(top_artist_dist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num users: 3000\n"
     ]
    }
   ],
   "source": [
    "# read in users\n",
    "# user file is just user_id and their mainstreaminess value \n",
    "low_users = pd.read_csv(low_user_file, sep=',').set_index('user_id')\n",
    "medium_users = pd.read_csv(medium_user_file, sep=',').set_index('user_id')\n",
    "high_users = pd.read_csv(high_user_file, sep=',').set_index('user_id')\n",
    "num_users = len(low_users) + len(medium_users) + len(high_users)\n",
    "print('Num users: ' + str(num_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Users From Each Popularity Group & Their 10 Most Listened To Artists "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (For Analysis of Streaming Service Algorithmic Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>artist</th>\n",
       "      <th>listens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1513514</th>\n",
       "      <td>42845367</td>\n",
       "      <td>495</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513639</th>\n",
       "      <td>42845367</td>\n",
       "      <td>15624</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513657</th>\n",
       "      <td>42845367</td>\n",
       "      <td>27107</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513483</th>\n",
       "      <td>42845367</td>\n",
       "      <td>163</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513485</th>\n",
       "      <td>42845367</td>\n",
       "      <td>172</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user  artist  listens\n",
       "1513514  42845367     495      205\n",
       "1513639  42845367   15624      201\n",
       "1513657  42845367   27107      171\n",
       "1513483  42845367     163      156\n",
       "1513485  42845367     172      156"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toList = df_events.loc[df_events['user'] == 42845367].sort_values(by=['listens'], ascending=False)\n",
    "toList.head() #grabbing random users top 10 artists in 1 of the 3 groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[495, 15624, 27107, 163, 172, 4311, 8047, 7, 56, 245, 52741, 173416, 140, 1703, 3887, 42794, 1092, 2558, 54, 42835]\n"
     ]
    }
   ],
   "source": [
    "to_list_2 = toList['artist'].tolist()[:20]\n",
    "print(to_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating GAP of User Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low count (for check): 1000\n",
      "Med count (for check): 1000\n",
      "High count (for check): 1000\n",
      "0.04963328792099549\n",
      "0.054371119359489226\n",
      "0.06286028679778642\n"
     ]
    }
   ],
   "source": [
    "# placeholder vars for numerator of GAPp, waiting to be divided by sie of group\n",
    "low_gap_p = 0\n",
    "medium_gap_p = 0\n",
    "high_gap_p = 0\n",
    "total_gap_p = 0\n",
    "#Count for sanity check\n",
    "low_count = 0\n",
    "med_count = 0\n",
    "high_count = 0\n",
    "\n",
    "for u, df in df_events.groupby('user'):\n",
    "    \n",
    "    no_user_artists = len(set(df['artist'])) # profile size //number of artists in users profile\n",
    "    # get popularity (= fraction of users interacted with item) of user items and calculate average of it\n",
    "    user_pop_artist_fraq = sum(artist_dist[df['artist']] / num_users) / no_user_artists \n",
    "    \n",
    "    if u in low_users.index: # get user group-specific values\n",
    "        low_gap_p += user_pop_artist_fraq\n",
    "        low_count += 1\n",
    "    elif u in medium_users.index:\n",
    "        medium_gap_p += user_pop_artist_fraq\n",
    "        med_count += 1\n",
    "    else:\n",
    "        high_gap_p += user_pop_artist_fraq\n",
    "        high_count += 1\n",
    "\n",
    "total_gap_p = (low_gap_p + medium_gap_p + high_gap_p) / num_users\n",
    "low_gap_p /= len(low_users) # average popularity of items/artists in low/med/high groups (gap = group average popularity)\n",
    "medium_gap_p /= len(medium_users)\n",
    "high_gap_p /= len(high_users)\n",
    "print('Low count (for check): ' + str(low_count))\n",
    "print('Med count (for check): ' + str(med_count))\n",
    "print('High count (for check): ' + str(high_count))\n",
    "print(low_gap_p)\n",
    "print(medium_gap_p)\n",
    "print(high_gap_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max Scaling Ratings (Not Using Right Now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"scaled_df_events = pd.DataFrame()\\nfor user_id, group in df_events.groupby('user'):\\n    #print(group)\\n    min_listens = group['listens'].min()\\n    max_listens = group['listens'].max()\\n    std = (group['listens'] - min_listens) / (max_listens - min_listens)\\n    scaled_listens = std * 999 + 1\\n    to_replace = group.copy()\\n    to_replace['listens'] = scaled_listens\\n    #print(to_replace)\\n    scaled_df_events = scaled_df_events.append(to_replace)\\nscaled_df_events.head()  \""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Scale listening counts on a scale from 1-1000\n",
    "\"\"\"scaled_df_events = pd.DataFrame()\n",
    "for user_id, group in df_events.groupby('user'):\n",
    "    #print(group)\n",
    "    min_listens = group['listens'].min()\n",
    "    max_listens = group['listens'].max()\n",
    "    std = (group['listens'] - min_listens) / (max_listens - min_listens)\n",
    "    scaled_listens = std * 999 + 1\n",
    "    to_replace = group.copy()\n",
    "    to_replace['listens'] = scaled_listens\n",
    "    #print(to_replace)\n",
    "    scaled_df_events = scaled_df_events.append(to_replace)\n",
    "scaled_df_events.head()  \"\"\" \n",
    "#df_events.groupby('user').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Alternating Least Squares with the Implicit Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shaping the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize data to compare artist popularity vs recommendations\n",
    "#Normalize artist popularity\n",
    "normalized_artist_dist = pd.DataFrame(artist_dist)\n",
    "normalized_artist_dist.columns = ['listens']\n",
    "normalized_artist_dist['listens'] /= num_users\n",
    "normalized_artist_dist.head()\n",
    "\n",
    "num_times_recommended = pd.DataFrame(artist_dist)\n",
    "num_times_recommended.columns = ['Recommendation Frequency']\n",
    "num_times_recommended['Recommendation Frequency'] = 0\n",
    "#num_times_recommended.head()\n",
    "#normalized_artist_dist.head()\n",
    "#For potential use with graphs, not being used right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_events_file_to_lil_matrix():\n",
    "    # Artist to User matrix where artist_user_matrix[a, u] = num of times user u listened to artist a\n",
    "\n",
    "    # 352805, 3000 (total artists, users)\n",
    "    rows, cols = 352805, 3000\n",
    "    artist_user_matrix = scipy.sparse.lil_matrix((rows, cols), dtype=int)\n",
    "\n",
    "    # user\tartist\talbum\ttrack\ttimestamp\n",
    "\n",
    "    user_dict = {}  # simplify user id to 1, 2, 3 ...\n",
    "    artist_dict = {}\n",
    "\n",
    "    # populate with user_events_file\n",
    "    with open(\"/home/jimi/music_fairness/LFM-1b-surprise-and-implicit-analysis/ML_Base_Project/data/user_events.txt\", 'r') as fp:\n",
    "        line = fp.readline()\n",
    "        loop_count = 0\n",
    "        while line:\n",
    "            # get data from line\n",
    "            line = fp.readline()\n",
    "            parts = line.split(\"\\t\")\n",
    "\n",
    "            # end case\n",
    "            try:\n",
    "                user_id = int(parts[0])\n",
    "                artist_id = int(parts[1])\n",
    "            except ValueError:\n",
    "                print(\"end of file \" + line)\n",
    "                break\n",
    "\n",
    "            # use user_dict to shorten user_id\n",
    "            if user_id not in user_dict:\n",
    "                # this user_id has not bee seen\n",
    "                user_dict[user_id] = len(user_dict)\n",
    "            user_idx = user_dict[user_id]\n",
    "\n",
    "            # use track_dict to shorten track_id\n",
    "            if artist_id not in artist_dict:\n",
    "                # this user_id has not bee seen\n",
    "                artist_dict[artist_id] = len(artist_dict)\n",
    "            artist_idx = artist_dict[artist_id]\n",
    "\n",
    "            # increment count of user to track\n",
    "            artist_user_matrix[artist_idx, user_idx] += 1\n",
    "\n",
    "            # progress marker\n",
    "            loop_count = loop_count + 1\n",
    "            if loop_count % 10000000 == 0:\n",
    "                print(str(loop_count) + \"/ 28718087\")  # / num of lines in file\n",
    "\n",
    "    print(len(user_dict))\n",
    "    print(len(artist_dict))\n",
    "\n",
    "    # helpful dicts for converting artist and user count back to their ids\n",
    "    user_count_to_id_dict = {v: k for k, v in user_dict.items()}\n",
    "    artist_count_to_id_dict = {v: k for k, v in artist_dict.items()}\n",
    "\n",
    "    return artist_user_matrix, user_dict, artist_dict, user_count_to_id_dict, artist_count_to_id_dict\n",
    "\n",
    "\n",
    "## FUNCTIONS FROM from jmsteinw's blog post ## a_u_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train(ratings, pct_test=0.2):\n",
    "    '''\n",
    "    This function will take in the original user-item matrix and \"mask\" a percentage of the original ratings where a\n",
    "    user-item interaction has taken place for use as a test set. The test set will contain all of the original ratings,\n",
    "    while the training set replaces the specified percentage of them with a zero in the original ratings matrix.\n",
    "\n",
    "    parameters:\n",
    "\n",
    "    ratings - the original ratings matrix from which you want to generate a train/test set. Test is just a complete\n",
    "    copy of the original set. This is in the form of a sparse csr_matrix.\n",
    "\n",
    "    pct_test - The percentage of user-item interactions where an interaction took place that you want to mask in the\n",
    "    training set for later comparison to the test set, which contains all of the original ratings.\n",
    "\n",
    "    returns:\n",
    "\n",
    "    training_set - The altered version of the original data with a certain percentage of the user-item pairs\n",
    "    that originally had interaction set back to zero.\n",
    "\n",
    "    test_set - A copy of the original ratings matrix, unaltered, so it can be used to see how the rank order\n",
    "    compares with the actual interactions.\n",
    "\n",
    "    user_inds - From the randomly selected user-item indices, which user rows were altered in the training data.\n",
    "    This will be necessary later when evaluating the performance via AUC.\n",
    "    '''\n",
    "    test_set = ratings.copy()  # Make a copy of the original set to be the test set.\n",
    "    test_set[test_set != 0] = 1  # Store the test set as a binary preference matrix\n",
    "    training_set = ratings.copy()  # Make a copy of the original data we can alter as our training set.\n",
    "    nonzero_inds = training_set.nonzero()  # Find the indices in the ratings data where an interaction exists\n",
    "    nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1]))  # Zip these pairs together of user,item index into list\n",
    "    random.seed(0)  # Set the random seed to zero for reproducibility\n",
    "    num_samples = int(\n",
    "        np.ceil(pct_test * len(nonzero_pairs)))  # Round the number of samples needed to the nearest integer\n",
    "    samples = random.sample(nonzero_pairs, num_samples)  # Sample a random number of user-item pairs without replacement\n",
    "    user_inds = [index[0] for index in samples]  # Get the user row indices\n",
    "    item_inds = [index[1] for index in samples]  # Get the item column indices\n",
    "    training_set[user_inds, item_inds] = 0  # Assign all of the randomly chosen user-item pairs to zero\n",
    "    training_set.eliminate_zeros()  # Get rid of zeros in sparse array storage after update to save space\n",
    "    return training_set, test_set, list(set(user_inds))  # Output the unique list of user rows that were altered\n",
    "\n",
    "\n",
    "# Usage\n",
    "# a_to_u_train, a_to_u_test, altered_users = make_train(csr_sparse_matrix, pct_test = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000/ 28718087\n",
      "20000000/ 28718087\n",
      "end of file \n",
      "3000\n",
      "352805\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "a_u_tuple = user_events_file_to_lil_matrix()\n",
    "a_u_matrix = a_u_tuple[0] #artist/user matrix\n",
    "back_to_user_id = a_u_tuple[3] #dictionary to convert smaller user ids to original\n",
    "back_to_artist_id = a_u_tuple[4]# same above but for artists\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "u_to_a_train, u_to_a_test, altered_users = make_train(a_u_matrix.T.tocsr(), pct_test=0.2) #makes training/test set\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:35:08-140421]  This method is deprecated. Please use the AlternatingLeastSquares class instead\n",
      "[15:35:08-140421]  GPU training requires factor size to be a multiple of 32. Increasing factors from 50 to 64.\n",
      "[15:35:08-140421]  OpenBLAS detected. Its highly recommend to set the environment variable 'export OPENBLAS_NUM_THREADS=1' to disable its internal multithreading\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ddc14c626a4a4a83cfce88a9310fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# split original matrix into user vector and artist vector through ALS\n",
    "user_vecs, artists_vecs = implicit.alternating_least_squares((u_to_a_train).astype('double'),iterations=50, factors=50)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7734145  0.9261155  0.50536424 0.83280516 0.9921204  0.84349346\n",
      " 0.86699915 0.72439003 0.8522642  0.24458975]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(user_vecs[0,:].dot(artists_vecs.T))[:10]) # gets first 5 recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Top N Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_artists_implicit(user_vecs, artists_vecs, mf_train, back_to_user_id,back_to_artist_id, num=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        user_vecs: the feature vector for users\n",
    "        mf_train: training matrix\n",
    "        artists_vecs: the feature vector for artists\n",
    "        back_to_user_id: dictionary to convert back to original user id\n",
    "        back_to_artist_id: dictionary to convert back to original artist id\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id), ...] of size n.\n",
    "    \"\"\"\n",
    "    top_artists_dict = defaultdict(list)\n",
    "        # creating dictionary where user id is the key, and the val is a list of tuples of the artist id and \n",
    "        # the rating it thinks the user would give it\n",
    "        \n",
    "    for user_id in range(len(user_vecs)):\n",
    "        \n",
    "        pref_vec = mf_train[user_id,:].toarray() # Get the ratings from the training set ratings matrix\n",
    "        pref_vec = pref_vec.reshape(-1)\n",
    "        pref_vec =  pref_vec + 1 # Add 1 to everything, so that artists not listened to yet are equal to 1\n",
    "        pref_vec[pref_vec > 1] = 0 # Make everything already purchased zero\n",
    "        \n",
    "        rec_vector = user_vecs[user_id,:].dot(artists_vecs.T) # Get dot product of user vector and all artist vectors\n",
    "        # Scale this recommendation vector between 0 and 1\n",
    "        \n",
    "        min_max = MinMaxScaler()\n",
    "        rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0] \n",
    "        recommend_vector = pref_vec*rec_vector_scaled \n",
    "        # Items already purchased have their recommendation multiplied by zero\n",
    "        \n",
    "        product_idx = np.argsort(recommend_vector)[::-1][:num] # Sort the indices of the items into order \n",
    "        # of best recommendations\n",
    "        rec_list = [] # start empty list to store items\n",
    "        actual_user_id = back_to_user_id[user_id]\n",
    "        #print(actual_user_id)\n",
    "        \n",
    "        for index in product_idx:\n",
    "            actual_artist_id = back_to_artist_id[index]\n",
    "            rec_list.append(actual_artist_id)\n",
    "            \n",
    "        top_artists_dict[actual_user_id] = rec_list\n",
    "    print(\"done\")\n",
    "    return top_artists_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "implixit_top_n = get_top_artists_implicit(user_vecs, artists_vecs, u_to_a_train,back_to_user_id, back_to_artist_id,num = 10) \n",
    "#gets top n artists for all the users in the dataset\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2908, 15483, 6443, 43, 2836, 1998, 93, 3103, 6236, 2562]\n"
     ]
    }
   ],
   "source": [
    "print(implicit_top_n[6532902]) #testing out getting top recommendations for a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "print(artist_dist[6443]) #getting popularity of artist by indexing by id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating GAPr and Delta GAP for ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Train and test the four algorithms on our data set\n",
    "low_gap_r_list = []\n",
    "medium_gap_r_list = []\n",
    "high_gap_r_list = []\n",
    "total_gap_r_list= []\n",
    "#keeps track of the GAPr of each of the algs\n",
    "\n",
    "alg_recommendations = [] #Keeps track of how many times each artist was recommended\n",
    "\n",
    "#Will put for loop here with training, but for now we already have predictions\n",
    "    \n",
    "num_times_recommended = pd.DataFrame(artist_dist)\n",
    "num_times_recommended.columns = ['Recommendation Frequency']\n",
    "num_times_recommended['Recommendation Frequency'] = 0 #resets for each algorithm\n",
    "    \n",
    "    \n",
    "# resets to 0 before calculating for new alg\n",
    "low_gap_r = 0\n",
    "medium_gap_r = 0\n",
    "high_gap_r = 0\n",
    "    \n",
    "#keeps track of group size since it is unpredictable since the test set is selected randomly\n",
    "num_low_users = 0\n",
    "num_medium_users = 0\n",
    "num_high_users = 0\n",
    "    \n",
    "#keeps track of the mean absolute error of the current alg\n",
    "low_mae = 0\n",
    "medium_mae = 0\n",
    "high_mae = 0\n",
    "top_artists = implicit_top_n #Gets the top N recommendations for each user\n",
    "    \n",
    "\n",
    "for user_id, ratings in top_artists.items():\n",
    "    artist_id_list = [] #user profile size to compute top fraction in GAPr\n",
    "    for artist_id in ratings:\n",
    "            \n",
    "        artist_id_list.append(artist_id)\n",
    "        num_times_recommended.loc[artist_id] += 1 #increments the number of times the artist was recommended\n",
    "\n",
    "    gap = sum(artist_dist[artist_id_list]/ num_users) / len(artist_id_list) #fraction in numerator for GAPr\n",
    "    #print(\"gap:\" ,gap)\n",
    "    if user_id in low_users.index: #summation of all of the top fractions to compute numerator for GAPr\n",
    "        low_gap_r += gap\n",
    "        num_low_users += 1 #increments group size to get the denominator to compute GAPr\n",
    "\n",
    "    elif user_id in medium_users.index:\n",
    "        medium_gap_r += gap\n",
    "        num_medium_users += 1\n",
    "\n",
    "    elif user_id in high_users.index:\n",
    "        high_gap_r += gap\n",
    "        num_high_users += 1\n",
    "alg_recommendations.append(num_times_recommended)\n",
    "            \n",
    "total_gap_r = (low_gap_r + medium_gap_r + high_gap_r) / (num_low_users + num_medium_users + num_high_users)\n",
    "low_gap_r = low_gap_r / num_low_users #Computing GAPr for each group size for each algorithm\n",
    "medium_gap_r = medium_gap_r / num_medium_users\n",
    "high_gap_r = high_gap_r / num_high_users\n",
    "\n",
    "total_gap_r_list.append(total_gap_r)\n",
    "low_gap_r_list.append(low_gap_r)\n",
    "medium_gap_r_list.append(medium_gap_r)\n",
    "high_gap_r_list.append(high_gap_r)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5132151940425258\n",
      "1.7332807395549104\n",
      "1.7789633524148147\n",
      "1.6850323399738416\n"
     ]
    }
   ],
   "source": [
    "#Computes change in GAP for each RecSys alg\n",
    "delta_gap_low_list = []\n",
    "delta_gap_medium_list = []\n",
    "delta_gap_high_list = []\n",
    "delta_gap_total_list = []\n",
    "\n",
    "for i in range(len(low_gap_r_list)):\n",
    "    delta_gap_low = ((low_gap_r_list[i] - low_gap_p) / low_gap_p)\n",
    "    delta_gap_medium = ((medium_gap_r_list[i] - medium_gap_p) / medium_gap_p)\n",
    "    delta_gap_high = ((high_gap_r_list[i] - high_gap_p) / high_gap_p)\n",
    "    delta_gap_total = ((total_gap_r_list[i] - total_gap_p) / total_gap_p)\n",
    "    \n",
    "    delta_gap_low_list.append(delta_gap_low)\n",
    "    delta_gap_medium_list.append(delta_gap_medium)\n",
    "    delta_gap_high_list.append(delta_gap_high)\n",
    "    delta_gap_total_list.append(delta_gap_total)\n",
    "    \n",
    "print(delta_gap_low_list[0])\n",
    "\n",
    "print(delta_gap_medium_list[0])\n",
    "\n",
    "print(delta_gap_high_list[0])\n",
    "\n",
    "print(delta_gap_total_list[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating AUC for ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_score(predictions, test):\n",
    "    '''\n",
    "    This simple function will output the area under the curve using sklearn's metrics.\n",
    "\n",
    "    parameters:\n",
    "\n",
    "    - predictions: your prediction output\n",
    "\n",
    "    - test: the actual target result you are comparing to\n",
    "\n",
    "    returns:\n",
    "\n",
    "    - AUC (area under the Receiver Operating Characterisic curve)\n",
    "    '''\n",
    "    # shuffle list of predictions (shuffle function)\n",
    "    # shuffling dissassociates link to artist - > roc of .5 (random)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test, predictions)\n",
    "    return metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_auc(training_set, altered_users, predictions, test_set):\n",
    "    '''\n",
    "    This function will calculate the mean AUC by user for any user that had their user-item matrix altered.\n",
    "\n",
    "    parameters:\n",
    "\n",
    "    training_set - The training set resulting from make_train, where a certain percentage of the original\n",
    "    user/item interactions are reset to zero to hide them from the model\n",
    "\n",
    "    predictions - The matrix of your predicted ratings for each user/item pair as output from the implicit MF.\n",
    "    These should be stored in a list, with user vectors as item zero and item vectors as item one.\n",
    "\n",
    "    altered_users - The indices of the users where at least one user/item pair was altered from make_train function\n",
    "\n",
    "    test_set - The test set constucted earlier from make_train function\n",
    "\n",
    "    returns:\n",
    "\n",
    "    The mean AUC (area under the Receiver Operator Characteristic curve) of the test set only on user-item interactions\n",
    "    there were originally zero to test ranking ability in addition to the most popular items as a benchmark.\n",
    "    '''\n",
    "\n",
    "    store_auc = []  # An empty list to store the AUC for each user that had an item removed from the training set\n",
    "    popularity_auc = []  # To store popular AUC scores\n",
    "    pop_items = np.array(test_set.sum(axis=0)).reshape(-1)  # Get sum of item iteractions to find most popular\n",
    "    item_vecs = predictions[1]\n",
    "    for user in altered_users:  # Iterate through each user that had an item altered\n",
    "        training_row = training_set[user, :].toarray().reshape(-1)  # Get the training set row\n",
    "        zero_inds = np.where(training_row == 0)  # Find where the interaction had not yet occurred\n",
    "        # Get the predicted values based on our user/item vectors\n",
    "        user_vec = predictions[0][user, :]\n",
    "        pred = user_vec.dot(item_vecs).toarray()[0, zero_inds].reshape(-1)\n",
    "        # Get only the items that were originally zero\n",
    "        # Select all ratings from the MF prediction for this user that originally had no iteraction\n",
    "        actual = test_set[user, :].toarray()[0, zero_inds].reshape(-1)\n",
    "        # Select the binarized yes/no interaction pairs from the original full data\n",
    "        # that align with the same pairs in training\n",
    "        pop = pop_items[zero_inds]  # Get the item popularity for our chosen items\n",
    "        curr_auc_score = auc_score(pred, actual)\n",
    "        store_auc.append(curr_auc_score)  # Calculate AUC for the given user and store\n",
    "        curr_pop_score = auc_score(pop, actual)\n",
    "        popularity_auc.append(curr_pop_score)  # Calculate AUC using most popular and score\n",
    "        # print(user, \"\\t\", curr_auc_score , \"\\t\", curr_pop_score)\n",
    "    # End users iteration\n",
    "\n",
    "    return float('%.3f' % np.mean(store_auc)), float('%.3f' % np.mean(popularity_auc))\n",
    "    # Return the mean AUC rounded to three decimal places for both test and popularity benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_auc, pop_auc = calc_mean_auc(u_to_a_train, altered_users,[sparse.csr_matrix(user_vecs), sparse.csr_matrix(artists_vecs.T)],\n",
    "                                             u_to_a_test)\n",
    "print(rec_auc)\n",
    "print(pop_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rectorch Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "data_events = df_events.copy(deep=True)\n",
    "data_events.to_csv('data_events.csv',index=False,header=False )\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_data_test = {\n",
    "    \"processing\": {\n",
    "        \"data_path\": \"data_events.csv\",\n",
    "        \"threshold\": 0,\n",
    "        \"separator\": \",\",\n",
    "        \"header\": None,\n",
    "        \"u_min\": 50,\n",
    "        \"i_min\": 50\n",
    "    },\n",
    "    \"splitting\": {\n",
    "        \"split_type\": \"horizontal\",\n",
    "        \"sort_by\": None,\n",
    "        \"seed\": 98765,\n",
    "        \"shuffle\": True,\n",
    "        \"valid_size\": 0.1,\n",
    "        \"test_size\": 0.1,\n",
    "        \"test_prop\": 0.2\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "cfg_data_full = {\n",
    "    \"processing\": {\n",
    "        \"data_path\": \"data_events.csv\",\n",
    "        \"threshold\": 0,\n",
    "        \"separator\": \",\",\n",
    "        \"header\": None,\n",
    "        \"u_min\": 0,\n",
    "        \"i_min\": 0\n",
    "    },\n",
    "    \"splitting\": {\n",
    "        \"split_type\": \"horizontal\",\n",
    "        \"sort_by\": None,\n",
    "        \"seed\": 98765,\n",
    "        \"shuffle\": True,\n",
    "        \"valid_size\": 0.1,\n",
    "        \"test_size\": 0.1,\n",
    "        \"test_prop\": 0.2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:30:55-280421]  Reading raw data file data_events.csv.\n",
      "[18:31:00-280421]  Applying filtering.\n",
      "[18:31:00-280421]  Filtered 988317 ratings.\n",
      "[18:31:00-280421]  Shuffling data.\n",
      "[18:31:00-280421]  Creating training, validation and test set.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(n_users=2804, n_items=5131, n_ratings=767044)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DataProcessing(cfg_data_test).process_and_split()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:31:01-280421]  Reading raw data file data_events.csv.\n",
      "[18:31:06-280421]  Applying filtering.\n",
      "[18:31:06-280421]  Shuffling data.\n",
      "[18:31:06-280421]  Creating training, validation and test set.\n",
      "[18:31:07-280421]  Skipped 25021 ratings in validation set.\n",
      "[18:31:07-280421]  Skipped 24878 ratings in test set.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(n_users=3000, n_items=304953, n_ratings=1705462)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "datasetFull = DataProcessing(cfg_data_full).process_and_split()\n",
    "datasetFull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_sampler = SparseDummySampler(dataset, mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search on SLIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:54:58-270421]  | item 513/5131 | ms/user 3.70 |\n",
      "[18:55:00-270421]  | item 1026/5131 | ms/user 3.62 |\n",
      "[18:55:02-270421]  | item 1539/5131 | ms/user 3.64 |\n",
      "[18:55:03-270421]  | item 2052/5131 | ms/user 3.63 |\n",
      "[18:55:05-270421]  | item 2565/5131 | ms/user 3.65 |\n",
      "[18:55:07-270421]  | item 3078/5131 | ms/user 3.64 |\n",
      "[18:55:09-270421]  | item 3591/5131 | ms/user 3.66 |\n",
      "[18:55:11-270421]  | item 4104/5131 | ms/user 3.65 |\n",
      "[18:55:13-270421]  | item 4617/5131 | ms/user 3.65 |\n",
      "[18:55:15-270421]  | item 5130/5131 | ms/user 3.64 |\n",
      "[18:55:15-270421]  | training complete | total training time 18.74 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "0.5\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:55:17-270421]  | item 513/5131 | ms/user 3.64 |\n",
      "[18:55:18-270421]  | item 1026/5131 | ms/user 3.64 |\n",
      "[18:55:20-270421]  | item 1539/5131 | ms/user 3.64 |\n",
      "[18:55:22-270421]  | item 2052/5131 | ms/user 3.65 |\n",
      "[18:55:24-270421]  | item 2565/5131 | ms/user 3.65 |\n",
      "[18:55:26-270421]  | item 3078/5131 | ms/user 3.64 |\n",
      "[18:55:28-270421]  | item 3591/5131 | ms/user 3.64 |\n",
      "[18:55:30-270421]  | item 4104/5131 | ms/user 3.65 |\n",
      "[18:55:32-270421]  | item 4617/5131 | ms/user 3.72 |\n",
      "[18:55:33-270421]  | item 5130/5131 | ms/user 3.69 |\n",
      "[18:55:33-270421]  | training complete | total training time 18.78 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "0.5\n",
      "1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:55:35-270421]  | item 513/5131 | ms/user 3.65 |\n",
      "[18:55:37-270421]  | item 1026/5131 | ms/user 3.66 |\n",
      "[18:55:39-270421]  | item 1539/5131 | ms/user 3.70 |\n",
      "[18:55:41-270421]  | item 2052/5131 | ms/user 3.66 |\n",
      "[18:55:43-270421]  | item 2565/5131 | ms/user 3.65 |\n",
      "[18:55:45-270421]  | item 3078/5131 | ms/user 3.72 |\n",
      "[18:55:47-270421]  | item 3591/5131 | ms/user 3.61 |\n",
      "[18:55:49-270421]  | item 4104/5131 | ms/user 3.59 |\n",
      "[18:55:50-270421]  | item 4617/5131 | ms/user 3.56 |\n",
      "[18:55:52-270421]  | item 5130/5131 | ms/user 3.56 |\n",
      "[18:55:52-270421]  | training complete | total training time 18.68 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "0.5\n",
      "2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:55:54-270421]  | item 513/5131 | ms/user 3.64 |\n",
      "[18:55:56-270421]  | item 1026/5131 | ms/user 3.61 |\n",
      "[18:55:58-270421]  | item 1539/5131 | ms/user 3.60 |\n",
      "[18:56:00-270421]  | item 2052/5131 | ms/user 3.61 |\n",
      "[18:56:02-270421]  | item 2565/5131 | ms/user 3.60 |\n",
      "[18:56:03-270421]  | item 3078/5131 | ms/user 3.60 |\n",
      "[18:56:05-270421]  | item 3591/5131 | ms/user 3.61 |\n",
      "[18:56:07-270421]  | item 4104/5131 | ms/user 3.60 |\n",
      "[18:56:09-270421]  | item 4617/5131 | ms/user 3.61 |\n",
      "[18:56:11-270421]  | item 5130/5131 | ms/user 3.69 |\n",
      "[18:56:11-270421]  | training complete | total training time 18.58 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "0.5\n",
      "2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:56:13-270421]  | item 513/5131 | ms/user 3.60 |\n",
      "[18:56:15-270421]  | item 1026/5131 | ms/user 3.60 |\n",
      "[18:56:16-270421]  | item 1539/5131 | ms/user 3.63 |\n",
      "[18:56:18-270421]  | item 2052/5131 | ms/user 3.60 |\n",
      "[18:56:20-270421]  | item 2565/5131 | ms/user 3.63 |\n",
      "[18:56:22-270421]  | item 3078/5131 | ms/user 3.60 |\n",
      "[18:56:24-270421]  | item 3591/5131 | ms/user 3.63 |\n",
      "[18:56:26-270421]  | item 4104/5131 | ms/user 3.63 |\n",
      "[18:56:28-270421]  | item 4617/5131 | ms/user 3.64 |\n",
      "[18:56:29-270421]  | item 5130/5131 | ms/user 3.63 |\n",
      "[18:56:29-270421]  | training complete | total training time 18.60 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "0.5\n",
      "3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:56:31-270421]  | item 513/5131 | ms/user 3.63 |\n",
      "[18:56:33-270421]  | item 1026/5131 | ms/user 3.64 |\n",
      "[18:56:35-270421]  | item 1539/5131 | ms/user 3.63 |\n",
      "[18:56:37-270421]  | item 2052/5131 | ms/user 3.62 |\n",
      "[18:56:39-270421]  | item 2565/5131 | ms/user 3.63 |\n",
      "[18:56:41-270421]  | item 3078/5131 | ms/user 3.62 |\n",
      "[18:56:43-270421]  | item 3591/5131 | ms/user 3.62 |\n",
      "[18:56:44-270421]  | item 4104/5131 | ms/user 3.63 |\n",
      "[18:56:46-270421]  | item 4617/5131 | ms/user 3.62 |\n",
      "[18:56:48-270421]  | item 5130/5131 | ms/user 3.64 |\n",
      "[18:56:48-270421]  | training complete | total training time 18.64 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "0.5\n",
      "3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:56:50-270421]  | item 513/5131 | ms/user 3.62 |\n",
      "[18:56:52-270421]  | item 1026/5131 | ms/user 3.61 |\n",
      "[18:56:54-270421]  | item 1539/5131 | ms/user 3.63 |\n",
      "[18:56:56-270421]  | item 2052/5131 | ms/user 3.59 |\n",
      "[18:56:57-270421]  | item 2565/5131 | ms/user 3.57 |\n",
      "[18:56:59-270421]  | item 3078/5131 | ms/user 3.58 |\n",
      "[18:57:01-270421]  | item 3591/5131 | ms/user 3.53 |\n",
      "[18:57:03-270421]  | item 4104/5131 | ms/user 3.55 |\n",
      "[18:57:05-270421]  | item 4617/5131 | ms/user 3.63 |\n",
      "[18:57:07-270421]  | item 5130/5131 | ms/user 3.56 |\n",
      "[18:57:07-270421]  | training complete | total training time 18.43 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "0.5\n",
      "4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:57:09-270421]  | item 513/5131 | ms/user 3.59 |\n",
      "[18:57:10-270421]  | item 1026/5131 | ms/user 3.66 |\n",
      "[18:57:12-270421]  | item 1539/5131 | ms/user 3.72 |\n",
      "[18:57:14-270421]  | item 2052/5131 | ms/user 3.68 |\n",
      "[18:57:16-270421]  | item 2565/5131 | ms/user 3.73 |\n",
      "[18:57:18-270421]  | item 3078/5131 | ms/user 3.82 |\n",
      "[18:57:20-270421]  | item 3591/5131 | ms/user 3.57 |\n",
      "[18:57:22-270421]  | item 4104/5131 | ms/user 3.59 |\n",
      "[18:57:24-270421]  | item 4617/5131 | ms/user 3.94 |\n",
      "[18:57:26-270421]  | item 5130/5131 | ms/user 4.04 |\n",
      "[18:57:26-270421]  | training complete | total training time 19.19 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "0.5\n",
      "4.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:57:28-270421]  | item 513/5131 | ms/user 3.84 |\n",
      "[18:57:30-270421]  | item 1026/5131 | ms/user 3.80 |\n",
      "[18:57:32-270421]  | item 1539/5131 | ms/user 3.69 |\n",
      "[18:57:34-270421]  | item 2052/5131 | ms/user 3.70 |\n",
      "[18:57:36-270421]  | item 2565/5131 | ms/user 3.80 |\n",
      "[18:57:38-270421]  | item 3078/5131 | ms/user 4.06 |\n",
      "[18:57:40-270421]  | item 3591/5131 | ms/user 3.85 |\n",
      "[18:57:42-270421]  | item 4104/5131 | ms/user 3.85 |\n",
      "[18:57:44-270421]  | item 4617/5131 | ms/user 3.81 |\n",
      "[18:57:45-270421]  | item 5130/5131 | ms/user 3.59 |\n",
      "[18:57:45-270421]  | training complete | total training time 19.53 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "0.5\n",
      "5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:57:47-270421]  | item 513/5131 | ms/user 3.68 |\n",
      "[18:57:49-270421]  | item 1026/5131 | ms/user 3.71 |\n",
      "[18:57:51-270421]  | item 1539/5131 | ms/user 3.78 |\n",
      "[18:57:53-270421]  | item 2052/5131 | ms/user 3.75 |\n",
      "[18:57:55-270421]  | item 2565/5131 | ms/user 3.69 |\n",
      "[18:57:57-270421]  | item 3078/5131 | ms/user 3.64 |\n",
      "[18:57:59-270421]  | item 3591/5131 | ms/user 3.63 |\n",
      "[18:58:01-270421]  | item 4104/5131 | ms/user 3.59 |\n",
      "[18:58:02-270421]  | item 4617/5131 | ms/user 3.65 |\n",
      "[18:58:04-270421]  | item 5130/5131 | ms/user 3.64 |\n",
      "[18:58:04-270421]  | training complete | total training time 18.89 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "1.0\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:58:06-270421]  | item 513/5131 | ms/user 3.64 |\n",
      "[18:58:08-270421]  | item 1026/5131 | ms/user 3.69 |\n",
      "[18:58:10-270421]  | item 1539/5131 | ms/user 3.67 |\n",
      "[18:58:12-270421]  | item 2052/5131 | ms/user 3.74 |\n",
      "[18:58:14-270421]  | item 2565/5131 | ms/user 3.70 |\n",
      "[18:58:16-270421]  | item 3078/5131 | ms/user 3.69 |\n",
      "[18:58:18-270421]  | item 3591/5131 | ms/user 3.63 |\n",
      "[18:58:20-270421]  | item 4104/5131 | ms/user 3.65 |\n",
      "[18:58:21-270421]  | item 4617/5131 | ms/user 3.64 |\n",
      "[18:58:23-270421]  | item 5130/5131 | ms/user 3.65 |\n",
      "[18:58:23-270421]  | training complete | total training time 18.86 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:58:25-270421]  | item 513/5131 | ms/user 3.68 |\n",
      "[18:58:27-270421]  | item 1026/5131 | ms/user 3.73 |\n",
      "[18:58:29-270421]  | item 1539/5131 | ms/user 3.82 |\n",
      "[18:58:31-270421]  | item 2052/5131 | ms/user 3.63 |\n",
      "[18:58:33-270421]  | item 2565/5131 | ms/user 3.64 |\n",
      "[18:58:35-270421]  | item 3078/5131 | ms/user 3.68 |\n",
      "[18:58:37-270421]  | item 3591/5131 | ms/user 3.67 |\n",
      "[18:58:38-270421]  | item 4104/5131 | ms/user 3.64 |\n",
      "[18:58:40-270421]  | item 4617/5131 | ms/user 3.66 |\n",
      "[18:58:42-270421]  | item 5130/5131 | ms/user 3.69 |\n",
      "[18:58:42-270421]  | training complete | total training time 18.92 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "1.0\n",
      "1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:58:44-270421]  | item 513/5131 | ms/user 3.79 |\n",
      "[18:58:46-270421]  | item 1026/5131 | ms/user 3.73 |\n",
      "[18:58:48-270421]  | item 1539/5131 | ms/user 3.65 |\n",
      "[18:58:50-270421]  | item 2052/5131 | ms/user 3.65 |\n",
      "[18:58:52-270421]  | item 2565/5131 | ms/user 3.67 |\n",
      "[18:58:54-270421]  | item 3078/5131 | ms/user 3.69 |\n",
      "[18:58:56-270421]  | item 3591/5131 | ms/user 3.70 |\n",
      "[18:58:57-270421]  | item 4104/5131 | ms/user 3.75 |\n",
      "[18:58:59-270421]  | item 4617/5131 | ms/user 3.66 |\n",
      "[18:59:01-270421]  | item 5130/5131 | ms/user 3.68 |\n",
      "[18:59:01-270421]  | training complete | total training time 19.00 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "1.0\n",
      "2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:59:03-270421]  | item 513/5131 | ms/user 3.69 |\n",
      "[18:59:05-270421]  | item 1026/5131 | ms/user 3.70 |\n",
      "[18:59:07-270421]  | item 1539/5131 | ms/user 3.70 |\n",
      "[18:59:09-270421]  | item 2052/5131 | ms/user 3.74 |\n",
      "[18:59:11-270421]  | item 2565/5131 | ms/user 3.66 |\n",
      "[18:59:13-270421]  | item 3078/5131 | ms/user 3.54 |\n",
      "[18:59:14-270421]  | item 3591/5131 | ms/user 3.57 |\n",
      "[18:59:16-270421]  | item 4104/5131 | ms/user 3.58 |\n",
      "[18:59:18-270421]  | item 4617/5131 | ms/user 3.69 |\n",
      "[18:59:20-270421]  | item 5130/5131 | ms/user 3.72 |\n",
      "[18:59:20-270421]  | training complete | total training time 18.80 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "1.0\n",
      "2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:59:22-270421]  | item 513/5131 | ms/user 3.67 |\n",
      "[18:59:24-270421]  | item 1026/5131 | ms/user 3.62 |\n",
      "[18:59:26-270421]  | item 1539/5131 | ms/user 3.62 |\n",
      "[18:59:28-270421]  | item 2052/5131 | ms/user 3.63 |\n",
      "[18:59:29-270421]  | item 2565/5131 | ms/user 3.62 |\n",
      "[18:59:31-270421]  | item 3078/5131 | ms/user 3.63 |\n",
      "[18:59:33-270421]  | item 3591/5131 | ms/user 3.62 |\n",
      "[18:59:35-270421]  | item 4104/5131 | ms/user 3.63 |\n",
      "[18:59:37-270421]  | item 4617/5131 | ms/user 3.63 |\n",
      "[18:59:39-270421]  | item 5130/5131 | ms/user 3.62 |\n",
      "[18:59:39-270421]  | training complete | total training time 18.65 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "1.0\n",
      "3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:59:41-270421]  | item 513/5131 | ms/user 3.65 |\n",
      "[18:59:43-270421]  | item 1026/5131 | ms/user 3.71 |\n",
      "[18:59:45-270421]  | item 1539/5131 | ms/user 3.67 |\n",
      "[18:59:46-270421]  | item 2052/5131 | ms/user 3.66 |\n",
      "[18:59:48-270421]  | item 2565/5131 | ms/user 3.64 |\n",
      "[18:59:50-270421]  | item 3078/5131 | ms/user 3.66 |\n",
      "[18:59:52-270421]  | item 3591/5131 | ms/user 3.65 |\n",
      "[18:59:54-270421]  | item 4104/5131 | ms/user 3.70 |\n",
      "[18:59:56-270421]  | item 4617/5131 | ms/user 3.63 |\n",
      "[18:59:58-270421]  | item 5130/5131 | ms/user 3.59 |\n",
      "[18:59:58-270421]  | training complete | total training time 18.79 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "1.0\n",
      "3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:00:00-270421]  | item 513/5131 | ms/user 3.61 |\n",
      "[19:00:01-270421]  | item 1026/5131 | ms/user 3.60 |\n",
      "[19:00:03-270421]  | item 1539/5131 | ms/user 3.61 |\n",
      "[19:00:05-270421]  | item 2052/5131 | ms/user 3.69 |\n",
      "[19:00:07-270421]  | item 2565/5131 | ms/user 3.71 |\n",
      "[19:00:09-270421]  | item 3078/5131 | ms/user 3.68 |\n",
      "[19:00:11-270421]  | item 3591/5131 | ms/user 3.66 |\n",
      "[19:00:13-270421]  | item 4104/5131 | ms/user 3.68 |\n",
      "[19:00:15-270421]  | item 4617/5131 | ms/user 3.68 |\n",
      "[19:00:17-270421]  | item 5130/5131 | ms/user 3.81 |\n",
      "[19:00:17-270421]  | training complete | total training time 18.88 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@50000': (0.0, 0.0)}\n",
      "0.0\n",
      "1.0\n",
      "4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:00:19-270421]  | item 513/5131 | ms/user 3.71 |\n",
      "[19:00:20-270421]  | item 1026/5131 | ms/user 3.75 |\n",
      "[19:00:22-270421]  | item 1539/5131 | ms/user 3.75 |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7d74e8de1b59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mslim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSLIM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_reg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_sampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msparse_sampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/models/baseline.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, verbose)\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0mtrain_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_pos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_pos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mnnz_coef_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_coef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/music_fairness/music_fairness/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    853\u001b[0m                           \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m                           \u001b[0mselection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m                           check_input=False)\n\u001b[0m\u001b[1;32m    856\u001b[0m             \u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_coef\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mdual_gaps_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_dual_gap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/music_fairness/music_fairness/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/music_fairness/music_fairness/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36menet_path\u001b[0;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mcoefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_alphas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         coefs = np.empty((n_outputs, n_features, n_alphas),\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lambda_vals =  np.array([0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3])\n",
    "beta_vals =  np.array([0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3])\n",
    "\n",
    "\n",
    "error = np.zeros([lambda_vals.size,beta_vals.size])\n",
    "\n",
    "sparse_sampler = SparseDummySampler(dataset, mode=\"train\")\n",
    "for i in range(len(lambda_vals)):\n",
    "    for j in range(len(beta_vals)):\n",
    "        sparse_sampler.train()\n",
    "        \n",
    "        print(lambda_vals[i])\n",
    "        print(beta_vals[j])\n",
    "        \n",
    "        slim = SLIM(l1_reg=lambda_vals[i], l2_reg=beta_vals[j])\n",
    "        slim.train(sparse_sampler)\n",
    "        \n",
    "        sparse_sampler.test()\n",
    "        \n",
    "        results = evaluate(slim, sparse_sampler, [\"ap@5000\"])\n",
    "        error[i,j] = collect_results(results).get(\"ap@5000\")[0]\n",
    "        print(collect_results(results))\n",
    "        print(collect_results(results).get(\"ap@5000\")[0])\n",
    "        \n",
    "print(error)\n",
    "        \n",
    "minError = np.argwhere(error == np.amax(error))\n",
    "Lambda = lambda_vals[(minError[0][0])]\n",
    "beta = beta_vals[(minError[0][1])]\n",
    "print(Lambda)\n",
    "print(beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SLIM on Partial Dataset with Tuned Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:26:59-090421]  | item 513/5131 | ms/user 13.48 |\n",
      "[13:27:06-090421]  | item 1026/5131 | ms/user 13.20 |\n",
      "[13:27:13-090421]  | item 1539/5131 | ms/user 13.07 |\n",
      "[13:27:19-090421]  | item 2052/5131 | ms/user 12.69 |\n",
      "[13:27:25-090421]  | item 2565/5131 | ms/user 12.41 |\n",
      "[13:27:32-090421]  | item 3078/5131 | ms/user 12.44 |\n",
      "[13:27:38-090421]  | item 3591/5131 | ms/user 12.23 |\n",
      "[13:27:44-090421]  | item 4104/5131 | ms/user 12.11 |\n",
      "[13:27:50-090421]  | item 4617/5131 | ms/user 11.97 |\n",
      "[13:27:56-090421]  | item 5130/5131 | ms/user 11.72 |\n",
      "[13:27:56-090421]  | training complete | total training time 64.35 s |\n"
     ]
    }
   ],
   "source": [
    "#Learned best Lambda = 0.0003 and best Beta = 0.03\n",
    "sparse_sampler.train()\n",
    "slim = SLIM(l1_reg=Lambda, l2_reg=beta)\n",
    "slim.train(sparse_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ap@5000': (0.1413211884342339, 0.08234902229175063)}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_sampler.test()\n",
    "results = evaluate(slim, sparse_sampler, [\"ap@5000\"])\n",
    "collect_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving/Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:28:22-090421]  Saving SLIM model to slimHorizontal...\n",
      "[13:28:22-090421]  Model saved!\n"
     ]
    }
   ],
   "source": [
    "slim.save_model(\"slimHorizontal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:24:27-280421]  Loading SLIM model from slimHorizontal...\n",
      "[17:24:27-280421]  Model loaded!\n"
     ]
    }
   ],
   "source": [
    "slim2 = SLIM(l1_reg=.0003, l2_reg=.03)\n",
    "slim2 = slim2.load_model(\"slimHorizontal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:24:45-280421]  Sampler must be in valid or test mode. Froced switch to test mode!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ap@5000': (0.1413211884342339, 0.08234902229175063)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate(slim2, sparse_sampler, [\"ap@5000\"])\n",
    "collect_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Predictions Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(model, sampler):\n",
    "    \"\"\"Returns the Predictions matrix for a rectorch model\n",
    "    Args:\n",
    "        model: The rectorch model to get predictions for\n",
    "        sampler: the sampler the model used\n",
    "        \n",
    "    Returns:\n",
    "        the predictions matrix for the model\n",
    "    \"\"\"\n",
    "    sampler.test()\n",
    "    for _, (data_input, ground_truth) in enumerate(sampler):\n",
    "        data_input, ground_truth = prepare_for_prediction(data_input, ground_truth)\n",
    "        predictions = model.predict(*data_input)[0].cpu().numpy()\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2804\n"
     ]
    }
   ],
   "source": [
    "predictions = getPredictions(slim2, sparse_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Top N Artists for SLIM (Works with Any Rectorch Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_artists(predictions, back_to_user_id,back_to_artist_id, num=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        Predictions: the Prediction vectors\n",
    "        back_to_user_id: dictionary to convert back to original user id\n",
    "        back_to_artist_id: dictionary to convert back to original artist id\n",
    "        num(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id), ...] of size n.\n",
    "    \"\"\"\n",
    "    top_artists_dict = defaultdict(list)\n",
    "        # creating dictionary where user id is the key, and the val is a list of tuples of the artist id and \n",
    "        # the rating it thinks the user would give it\n",
    "        \n",
    "    for user_id in range(len(predictions)):\n",
    "        \n",
    "        rec_vector = predictions[user_id,:].copy()\n",
    "        #print(rec_vector)\n",
    "        \n",
    "        recommend_vector = rec_vector\n",
    "        \n",
    "        product_idx = np.argsort(recommend_vector)[::-1][:num] # Sort the indices of the items into order \n",
    "        # of best recommendations\n",
    "        rec_list = [] # start empty list to store items\n",
    "        actual_user_id = back_to_user_id[user_id]\n",
    "        #print(actual_user_id)\n",
    "        \n",
    "        for index in product_idx:\n",
    "            actual_artist_id = back_to_artist_id[index]\n",
    "            rec_list.append(actual_artist_id)\n",
    "            \n",
    "        top_artists_dict[actual_user_id] = rec_list\n",
    "    print(\"done\")\n",
    "    return top_artists_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reversing the matrices given by the dataset so we can get the original artist and user ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reverse_matrices(dataset):\n",
    "    \"\"\"Simply reverses the dictionaries in a rectorch dataset to be used to \n",
    "    reference back to the original ids\"\"\"\n",
    "    id2u =  dict(map(reversed, dataset.u2id.items())) #dict for putting user ids back to normal\n",
    "    id2i = dict(map(reversed, dataset.i2id.items())) #dict for translating artist ids back to normal\n",
    "    return id2u, id2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2u, id2i = get_reverse_matrices(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "rectorch_top_n = get_top_artists(predictions, id2u, id2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2836, 17292, 6443, 17302, 35098, 7941, 7463, 1804, 15629, 18830]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rectorch_top_n[6532902]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating GAPr and Delta GAP for Rectorch Algorithms (SLIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcGapR(algs_to_test, samplers, dataset, artist_dist, get_top_artists, get_reverse_matrices, get_predictions):\n",
    "    \"\"\"Returns the list of GAPrs for all categories given a list of algorithms\n",
    "        ALGORITHMS MUST ALREADY BE TRAINED SEPARATELY TO CONSERVE TIME\n",
    "        Args:\n",
    "            algs_to_test: list of already trained models\n",
    "            samplers: the sampler used for each algorithm (must be same length as algs_to_test)\n",
    "            dataset: the rectorch dataset object that everything is trained on\n",
    "            artist_dist: pandas df of how many times each artist was listened to by unique users\n",
    "            get_top_artists: function to get top n artists for each alg\n",
    "            get_reverse_matrices: function to reverse matrices to get user and artist ids back to their original form\n",
    "            get_predictions: function to get predictions for the models of each alg\n",
    "            \n",
    "        Returns:\n",
    "            low_gap_r_list: list of GAPr for low pop group\n",
    "            medium_gap_r_list: list of GAPr for med pop group\n",
    "            high_gap_r_list: list of GAPr for high pop group\n",
    "            total_gap_r_list: list of GAPr across entire dataset\n",
    "            all indexed in the same order as algs_to_test\"\"\"\n",
    "    # Train and test the four algorithms on our data set\n",
    "    low_gap_r_list = []\n",
    "    medium_gap_r_list = []\n",
    "    high_gap_r_list = []\n",
    "    total_gap_r_list= []\n",
    "    #keeps track of the GAPr of each of the algs\n",
    "\n",
    "    alg_recommendations = [] #Keeps track of how many times each artist was recommended\n",
    "\n",
    "    #Will put for loop here with training, but for now we already have predictions\n",
    "    for i in range(len(algs_to_test)):   \n",
    "\n",
    "        num_times_recommended = pd.DataFrame(artist_dist)\n",
    "        num_times_recommended.columns = ['Recommendation Frequency']\n",
    "        num_times_recommended['Recommendation Frequency'] = 0 #resets for each algorithm\n",
    "\n",
    "\n",
    "        # resets to 0 before calculating for new alg\n",
    "        low_gap_r = 0\n",
    "        medium_gap_r = 0\n",
    "        high_gap_r = 0\n",
    "\n",
    "        #keeps track of group size since it is unpredictable since the test set is selected randomly\n",
    "        num_low_users = 0\n",
    "        num_medium_users = 0\n",
    "        num_high_users = 0\n",
    "\n",
    "        #keeps track of the mean absolute error of the current alg\n",
    "        low_mae = 0\n",
    "        medium_mae = 0\n",
    "        high_mae = 0\n",
    "\n",
    "        predictions = get_predictions(algs_to_test[i], samplers[i])\n",
    "        userBack, artistBack = get_reverse_matrices(dataset)\n",
    "        top_artists = get_top_artists(predictions, userBack, artistBack) #Gets the top N recommendations for each user\n",
    "\n",
    "\n",
    "        for user_id, ratings in top_artists.items():\n",
    "            artist_id_list = [] #user profile size to compute top fraction in GAPr\n",
    "            for artist_id in ratings:\n",
    "\n",
    "                artist_id_list.append(artist_id)\n",
    "                num_times_recommended.loc[artist_id] += 1 #increments the number of times the artist was recommended\n",
    "\n",
    "            gap = sum(artist_dist[artist_id_list]/ num_users) / len(artist_id_list) #fraction in numerator for GAPr\n",
    "            #print(\"gap:\" ,gap)\n",
    "            if user_id in low_users.index: #summation of all of the top fractions to compute numerator for GAPr\n",
    "                low_gap_r += gap\n",
    "                num_low_users += 1 #increments group size to get the denominator to compute GAPr\n",
    "\n",
    "            elif user_id in medium_users.index:\n",
    "                medium_gap_r += gap\n",
    "                num_medium_users += 1\n",
    "\n",
    "            elif user_id in high_users.index:\n",
    "                high_gap_r += gap\n",
    "                num_high_users += 1\n",
    "        alg_recommendations.append(num_times_recommended)\n",
    "\n",
    "        total_gap_r = (low_gap_r + medium_gap_r + high_gap_r) / (num_low_users + num_medium_users + num_high_users)\n",
    "        low_gap_r = low_gap_r / num_low_users #Computing GAPr for each group size for each algorithm\n",
    "        medium_gap_r = medium_gap_r / num_medium_users\n",
    "        high_gap_r = high_gap_r / num_high_users\n",
    "\n",
    "        total_gap_r_list.append(total_gap_r)\n",
    "        low_gap_r_list.append(low_gap_r)\n",
    "        medium_gap_r_list.append(medium_gap_r)\n",
    "        high_gap_r_list.append(high_gap_r)\n",
    "\n",
    "    return low_gap_r_list, medium_gap_r_list, high_gap_r_list, total_gap_r_list\n",
    "\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computes change in GAP for each RecSys alg\n",
    "def calc_delta_gap(gap_p_list, gap_r_list):\n",
    "    \"\"\"Computes delta gap of all recsys data recorded given the gap_r_list, should be in order of algs_list if used\n",
    "        with function above\n",
    "\n",
    "        Args:\n",
    "            gap_p_list: list of the computed gapPs (in order of low,med,high,total)\n",
    "            gap_r_list: list of the computed gapRs (in order of low,med,high,total)\n",
    "        Returns:\n",
    "            delta_gap_low_list: list of all computed delta gaps for all algs for the low user group\n",
    "            delta_gap_medium_list: list of all computed delta gaps for all algs for the med user group\n",
    "            delta_gap_high_list: list of all computed delta gaps for all algs for the high user group\n",
    "            delta_gap_total_list: list of all computed delta gaps for all algs for all users\n",
    "    \"\"\"\n",
    "    delta_gap_low_list = []\n",
    "    delta_gap_medium_list = []\n",
    "    delta_gap_high_list = []\n",
    "    delta_gap_total_list = []\n",
    "\n",
    "    low_gap_p = gap_p_list[0]\n",
    "    medium_gap_p = gap_p_list[1]\n",
    "    high_gap_p = gap_p_list[2]\n",
    "    total_gap_p = gap_p_list[3]\n",
    "\n",
    "    low_gap_r_list = gap_r_list[0]\n",
    "    medium_gap_r_list = gap_r_list[1]\n",
    "    high_gap_r_list = gap_r_list[2]\n",
    "    total_gap_r_list = gap_r_list[3]\n",
    "\n",
    "    for i in range(len(low_gap_r_list)):\n",
    "        delta_gap_low = ((low_gap_r_list[i] - low_gap_p) / low_gap_p)\n",
    "        delta_gap_medium = ((medium_gap_r_list[i] - medium_gap_p) / medium_gap_p)\n",
    "        delta_gap_high = ((high_gap_r_list[i] - high_gap_p) / high_gap_p)\n",
    "        delta_gap_total = ((total_gap_r_list[i] - total_gap_p) / total_gap_p)\n",
    "\n",
    "        delta_gap_low_list.append(delta_gap_low)\n",
    "        delta_gap_medium_list.append(delta_gap_medium)\n",
    "        delta_gap_high_list.append(delta_gap_high)\n",
    "        delta_gap_total_list.append(delta_gap_total)\n",
    "\n",
    "    \n",
    "    return delta_gap_low_list, delta_gap_medium_list, delta_gap_high_list, delta_gap_total_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "2.663266993343709\n",
      "2.720945436349978\n",
      "2.409118205863711\n",
      "2.5877539912773777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.663266993343709],\n",
       " [2.720945436349978],\n",
       " [2.409118205863711],\n",
       " [2.5877539912773777])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_gap_r_list, medium_gap_r_list,high_gap_r_list,total_gap_r_list = calcGapR([slim2],[sparse_sampler], dataset, artist_dist, get_top_artists, get_reverse_matrices, getPredictions)\n",
    "\n",
    "calc_delta_gap([low_gap_p, medium_gap_p,high_gap_p, total_gap_p],[low_gap_r_list,medium_gap_r_list,high_gap_r_list,total_gap_r_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_sampler_full = SparseDummySampler(datasetFull, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:24:04-140421]  | item 30495/304953 | ms/user 338.65 |\n",
      "[22:14:27-140421]  | item 60990/304953 | ms/user 335.23 |\n",
      "[01:06:36-150421]  | item 91485/304953 | ms/user 338.72 |\n",
      "[03:59:30-150421]  | item 121980/304953 | ms/user 340.17 |\n",
      "[06:52:59-150421]  | item 152475/304953 | ms/user 341.34 |\n",
      "[09:48:10-150421]  | item 182970/304953 | ms/user 344.68 |\n",
      "[12:44:32-150421]  | item 213465/304953 | ms/user 347.00 |\n",
      "[15:44:01-150421]  | item 243960/304953 | ms/user 353.15 |\n",
      "[18:44:57-150421]  | item 274455/304953 | ms/user 355.98 |\n",
      "[21:47:56-150421]  | item 304950/304953 | ms/user 360.02 |\n",
      "[21:48:00-150421]  | training complete | total training time 105362.39 s |\n",
      "/home/jimi/music_fairness/music_fairness/lib/python3.6/site-packages/rectorch/metrics.py:245: RuntimeWarning: invalid value encountered in multiply\n",
      "  num = pred_scores * ground_truth\n",
      "[21:50:09-150421]  Saving SLIM model to slim_full...\n",
      "[21:50:15-150421]  Model saved!\n"
     ]
    }
   ],
   "source": [
    "sparse_sampler_full.train()\n",
    "slimFull = SLIM(l1_reg=.0003, l2_reg=.03)\n",
    "slimFull.train(sparse_sampler_full)\n",
    "\n",
    "sparse_sampler_full.test()\n",
    "results = evaluate(slimFull, sparse_sampler_full, [\"ap@5000\"])\n",
    "collect_results(results)\n",
    "\n",
    "slimFull.save_model(\"slim_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:31:37-280421]  Loading SLIM model from slim_full...\n",
      "[18:31:42-280421]  Model loaded!\n"
     ]
    }
   ],
   "source": [
    "slimFull = SLIM(l1_reg=.0003, l2_reg=.03)\n",
    "slimFull = slimFull.load_model(\"slim_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "2.723715030009914\n",
      "2.5596250781661016\n",
      "2.141520294075726\n",
      "2.450969770363109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.723715030009914],\n",
       " [2.5596250781661016],\n",
       " [2.141520294075726],\n",
       " [2.450969770363109])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_gap_r_list, medium_gap_r_list,high_gap_r_list,total_gap_r_list = calcGapR([slimFull],[sparse_sampler_full], datasetFull, artist_dist, get_top_artists, get_reverse_matrices, getPredictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "calc_delta_gap([low_gap_p, medium_gap_p,high_gap_p, total_gap_p],[low_gap_r_list,medium_gap_r_list,high_gap_r_list,total_gap_r_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_sampler_full = ArrayDummySampler(datasetFull, mode=\"train\")\n",
    "array_sampler_test = ArrayDummySampler(dataset,mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:56:14-270421]  EASE - start tarining (lam=500.0000)\n",
      "[19:56:14-270421]  EASE - linear kernel computed\n",
      "[19:56:17-270421]  EASE - training complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ap@5000': (0.13180846894672282, 0.08078918435269586)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ease = EASE(500)\n",
    "ease.train(array_sampler_test)\n",
    "\n",
    "array_sampler_test.test()\n",
    "results = evaluate(ease, array_sampler_test, [\"ap@5000\"])\n",
    "collect_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:54:40-270421]  EASE - start tarining (lam=100.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:54:40-270421]  EASE - linear kernel computed\n",
      "[19:54:42-270421]  EASE - training complete\n",
      "[19:54:44-270421]  EASE - start tarining (lam=150.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.1179903148670593, 0.0778611649631584)}\n",
      "0.1179903148670593\n",
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:54:44-270421]  EASE - linear kernel computed\n",
      "[19:54:46-270421]  EASE - training complete\n",
      "[19:54:48-270421]  EASE - start tarining (lam=200.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.12337434998215327, 0.07900532355629937)}\n",
      "0.12337434998215327\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:54:48-270421]  EASE - linear kernel computed\n",
      "[19:54:50-270421]  EASE - training complete\n",
      "[19:54:51-270421]  EASE - start tarining (lam=250.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.126492318514919, 0.07963787725992458)}\n",
      "0.126492318514919\n",
      "250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:54:52-270421]  EASE - linear kernel computed\n",
      "[19:54:54-270421]  EASE - training complete\n",
      "[19:54:55-270421]  EASE - start tarining (lam=300.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.12857187513476598, 0.08012493077934621)}\n",
      "0.12857187513476598\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:54:56-270421]  EASE - linear kernel computed\n",
      "[19:54:58-270421]  EASE - training complete\n",
      "[19:54:59-270421]  EASE - start tarining (lam=350.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.1298890251835875, 0.08022766815696428)}\n",
      "0.1298890251835875\n",
      "350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:55:00-270421]  EASE - linear kernel computed\n",
      "[19:55:02-270421]  EASE - training complete\n",
      "[19:55:03-270421]  EASE - start tarining (lam=400.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.13084361445296902, 0.08063322956534863)}\n",
      "0.13084361445296902\n",
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:55:04-270421]  EASE - linear kernel computed\n",
      "[19:55:06-270421]  EASE - training complete\n",
      "[19:55:07-270421]  EASE - start tarining (lam=450.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.13139021994619446, 0.08065568987775001)}\n",
      "0.13139021994619446\n",
      "450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:55:07-270421]  EASE - linear kernel computed\n",
      "[19:55:10-270421]  EASE - training complete\n",
      "[19:55:11-270421]  EASE - start tarining (lam=500.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.13160641688534572, 0.08074255447480981)}\n",
      "0.13160641688534572\n",
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:55:11-270421]  EASE - linear kernel computed\n",
      "[19:55:14-270421]  EASE - training complete\n",
      "[19:55:15-270421]  EASE - start tarining (lam=550.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.13180846894672282, 0.08078918435269586)}\n",
      "0.13180846894672282\n",
      "550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:55:15-270421]  EASE - linear kernel computed\n",
      "[19:55:17-270421]  EASE - training complete\n",
      "[19:55:19-270421]  EASE - start tarining (lam=600.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.1316883755184622, 0.08061286693923611)}\n",
      "0.1316883755184622\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:55:19-270421]  EASE - linear kernel computed\n",
      "[19:55:21-270421]  EASE - training complete\n",
      "[19:55:23-270421]  EASE - start tarining (lam=650.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.13164560079163398, 0.08061008742781052)}\n",
      "0.13164560079163398\n",
      "650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:55:23-270421]  EASE - linear kernel computed\n",
      "[19:55:25-270421]  EASE - training complete\n",
      "[19:55:26-270421]  EASE - start tarining (lam=700.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.13141524492738188, 0.08018292350720423)}\n",
      "0.13141524492738188\n",
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:55:27-270421]  EASE - linear kernel computed\n",
      "[19:55:29-270421]  EASE - training complete\n",
      "[19:55:30-270421]  EASE - start tarining (lam=750.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.13113942966458147, 0.08014753181176151)}\n",
      "0.13113942966458147\n",
      "750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:55:31-270421]  EASE - linear kernel computed\n",
      "[19:55:33-270421]  EASE - training complete\n",
      "[19:55:35-270421]  EASE - start tarining (lam=800.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.13102866974516203, 0.08008700420087392)}\n",
      "0.13102866974516203\n",
      "800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:55:35-270421]  EASE - linear kernel computed\n",
      "[19:55:37-270421]  EASE - training complete\n",
      "[19:55:39-270421]  EASE - start tarining (lam=850.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.13073164716464822, 0.07976555845352133)}\n",
      "0.13073164716464822\n",
      "850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:55:39-270421]  EASE - linear kernel computed\n",
      "[19:55:41-270421]  EASE - training complete\n",
      "[19:55:42-270421]  EASE - start tarining (lam=900.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.13045312113764976, 0.07955909810241284)}\n",
      "0.13045312113764976\n",
      "900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:55:43-270421]  EASE - linear kernel computed\n",
      "[19:55:45-270421]  EASE - training complete\n",
      "[19:55:46-270421]  EASE - start tarining (lam=950.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.13017729393471678, 0.07946441590788961)}\n",
      "0.13017729393471678\n",
      "950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:55:47-270421]  EASE - linear kernel computed\n",
      "[19:55:49-270421]  EASE - training complete\n",
      "[19:55:50-270421]  EASE - start tarining (lam=1000.0000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.12984975869899804, 0.07938022028209238)}\n",
      "0.12984975869899804\n",
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:55:51-270421]  EASE - linear kernel computed\n",
      "[19:55:53-270421]  EASE - training complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap@5000': (0.1294967565823914, 0.07918547487211418)}\n",
      "0.1294967565823914\n",
      "[0.11799031 0.12337435 0.12649232 0.12857188 0.12988903 0.13084361\n",
      " 0.13139022 0.13160642 0.13180847 0.13168838 0.1316456  0.13141524\n",
      " 0.13113943 0.13102867 0.13073165 0.13045312 0.13017729 0.12984976\n",
      " 0.12949676]\n",
      "[500]\n"
     ]
    }
   ],
   "source": [
    "lambda_vals =  np.array([100,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000])\n",
    "\n",
    "error = np.zeros([lambda_vals.size])\n",
    "\n",
    "\n",
    "for i in range(len(lambda_vals)):\n",
    "        array_sampler_test.train()\n",
    "        \n",
    "        print(lambda_vals[i])\n",
    "        \n",
    "        ease = EASE(lambda_vals[i])\n",
    "        ease.train(array_sampler_test)\n",
    "        \n",
    "        array_sampler_test.test()\n",
    "        \n",
    "        results = evaluate(ease, array_sampler_test, [\"ap@5000\"])\n",
    "        error[i] = collect_results(results).get(\"ap@5000\")[0]\n",
    "        print(collect_results(results))\n",
    "        print(collect_results(results).get(\"ap@5000\")[0])\n",
    "        \n",
    "print(error)\n",
    "        \n",
    "minError = np.argwhere(error == np.amax(error))\n",
    "Lambda = lambda_vals[(minError[0])]\n",
    "print(Lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Research.ipynb to script\n",
      "[NbConvertApp] Writing 36219 bytes to Research.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Research.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500 is best lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = Random(datasetFull.n_items)\n",
    "rnd.train(array_sampler_full)\n",
    "pop = Popularity(datasetFull.n_items)\n",
    "pop.train(array_sampler_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "-0.9631437682399542\n",
      "-0.9670995676674233\n",
      "-0.9702689870631357\n",
      "-0.9671168964772376\n"
     ]
    }
   ],
   "source": [
    "low_gap_r_list, medium_gap_r_list,high_gap_r_list,total_gap_r_list = calcGapR([rnd,pop],[array_sampler_full,array_sampler_full], datasetFull, artist_dist, get_top_artists, get_reverse_matrices, getPredictions)\n",
    "d_gap_low, d_gap_med, d_gap_high, d_gap_tot = calc_delta_gap([low_gap_p, medium_gap_p,high_gap_p, total_gap_p],[low_gap_r_list,medium_gap_r_list,high_gap_r_list,total_gap_r_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rand \\t Pop\")\n",
    "print(d_gap_low[0] + \"\\t\" + d_gap_low[1])\n",
    "print(\"---\")\n",
    "print(d_gap_med[0] + \"\\t\" + d_gap_med[1])\n",
    "print(\"---\")\n",
    "print(d_gap_high[0] + \"\\t\" + d_gap_high[1])\n",
    "print(\"---\")\n",
    "print(d_gap_tot[0] + \"\\t\" + d_gap_tot[1])\n",
    "print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
