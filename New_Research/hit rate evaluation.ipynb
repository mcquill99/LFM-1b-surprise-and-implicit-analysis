{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "funky-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from rectorch.models import RecSysModel, TorchNNTrainer, AETrainer, VAE, MultiDAE, MultiVAE,\\\n",
    "#    CMultiVAE, EASE, CFGAN, ADMM_Slim, SVAE\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import scipy\n",
    "from scipy import sparse \n",
    "#from surprise import Reader, Dataset\n",
    "#from surprise.model_selection import train_test_split\n",
    "#from surprise import NormalPredictor, BaselineOnly, KNNBasic, KNNWithMeans, SVD, NMF, accuracy\n",
    "from collections import defaultdict\n",
    "from surprise import SVDpp, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import implicit\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "round-citizen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of user events: 28718087\n",
      "No. user-artist pairs: 1755361\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>artist</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1021445</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021445</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1021445</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1021445</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1021445</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user  artist  count\n",
       "0  1021445      12     43\n",
       "1  1021445      16      1\n",
       "2  1021445      28      7\n",
       "3  1021445      29      1\n",
       "4  1021445      46      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize data\n",
    "item_threshold = 1 # used to filter out user/artist pairs that have been \n",
    "                   #listened to less than the threshold number of times\n",
    "popular_artist_fraction = 0.2 # top cutoff for what we consider popular artists, in this case the top 20%\n",
    "\n",
    "user_events_file = \"./data/user_events.txt\"\n",
    "low_user_file = \"./data/low_main_users.txt\"\n",
    "medium_user_file = \"./data/medium_main_users.txt/\"\n",
    "high_user_file = \"./data/high_main_users.txt\"\n",
    "\n",
    "\n",
    "#read in user events file\n",
    "cols = ['user', 'artist', 'album', 'track', 'timestamp']\n",
    "df_events = pd.read_csv(user_events_file, sep='\\t', names=cols)\n",
    "print('No. of user events: ' + str(len(df_events)))\n",
    "df_events.head() # check it is all read in properly\n",
    "\n",
    "\n",
    "# create unique user-artist matrix\n",
    "df_events = df_events.groupby(['user', 'artist']).size().reset_index(name='count')\n",
    "print('No. user-artist pairs: ' + str(len(df_events)))\n",
    "# each row contains a unique user-artist pair, along with how many times the\n",
    "# user has listened to the artist\n",
    "df_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "controlling-freeware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000/ 28718087\n",
      "20000000/ 28718087\n",
      "end of file \n",
      "3000\n",
      "352805\n"
     ]
    }
   ],
   "source": [
    "# Artist to User matrix where artist_user_matrix[a, u] = num of times user u listened to artist a\n",
    "\n",
    "# 352805, 3000 (total artists, users)\n",
    "rows, cols = 352805, 3000\n",
    "artist_user_matrix = scipy.sparse.lil_matrix((rows, cols), dtype=int)\n",
    "\n",
    "# user\tartist\talbum\ttrack\ttimestamp\n",
    "\n",
    "user_dict = {} #simplify user id to 1, 2, 3 ...\n",
    "artist_dict = {}\n",
    "\n",
    "# populate with user_events_file\n",
    "with open(user_events_file, 'r') as fp:\n",
    "    line = fp.readline()\n",
    "    loop_count = 0\n",
    "    while line:\n",
    "        # get data from line\n",
    "        line = fp.readline()\n",
    "        parts = line.split(\"\\t\")\n",
    "        \n",
    "        # end case\n",
    "        try:\n",
    "            user_id = int(parts[0])\n",
    "            artist_id = int(parts[1])\n",
    "        except ValueError:\n",
    "            print(\"end of file \" + line)\n",
    "            break\n",
    "        \n",
    "        # use user_dict to shorten user_id\n",
    "        if user_id not in user_dict:\n",
    "            # this user_id has not bee seen\n",
    "            user_dict[user_id] = len(user_dict)\n",
    "        user_idx = user_dict[user_id]\n",
    "        \n",
    "        # use track_dict to shorten track_id\n",
    "        if artist_id not in artist_dict:\n",
    "            # this user_id has not bee seen\n",
    "            artist_dict[artist_id] = len(artist_dict)\n",
    "        artist_idx = artist_dict[artist_id]\n",
    "        \n",
    "        # increment count of user to track\n",
    "        artist_user_matrix[artist_idx, user_idx] += 1\n",
    "        \n",
    "        # progress marker\n",
    "        loop_count = loop_count + 1\n",
    "        if loop_count % 10000000 == 0:\n",
    "            print(str(loop_count) + \"/ 28718087\")  # / num of lines in file\n",
    "\n",
    "print(len(user_dict))\n",
    "print(len(artist_dict))\n",
    "\n",
    "# helpful dicts for converting artist and user count back to their ids\n",
    "user_count_to_id_dict ={v: k for k, v in user_dict.items()}\n",
    "artist_count_to_id_dict = {v: k for k, v in artist_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "grateful-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train(ratings, pct_test=0.2, return_samples=False):\n",
    "    '''\n",
    "    This function will take in the original user-item matrix and \"mask\" a percentage of the original ratings where a\n",
    "    user-item interaction has taken place for use as a test set. The test set will contain all of the original ratings,\n",
    "    while the training set replaces the specified percentage of them with a zero in the original ratings matrix.\n",
    "\n",
    "    parameters:\n",
    "\n",
    "    ratings - the original ratings matrix from which you want to generate a train/test set. Test is just a complete\n",
    "    copy of the original set. This is in the form of a sparse csr_matrix.\n",
    "\n",
    "    pct_test - The percentage of user-item interactions where an interaction took place that you want to mask in the\n",
    "    training set for later comparison to the test set, which contains all of the original ratings.\n",
    "\n",
    "    returns:\n",
    "\n",
    "    training_set - The altered version of the original data with a certain percentage of the user-item pairs\n",
    "    that originally had interaction set back to zero.\n",
    "\n",
    "    test_set - A copy of the original ratings matrix, unaltered, so it can be used to see how the rank order\n",
    "    compares with the actual interactions.\n",
    "\n",
    "    user_inds - From the randomly selected user-item indices, which user rows were altered in the training data.\n",
    "    This will be necessary later when evaluating the performance via AUC.\n",
    "    '''\n",
    "    test_set = ratings.copy()  # Make a copy of the original set to be the test set.\n",
    "    test_set[test_set != 0] = 1  # Store the test set as a binary preference matrix\n",
    "    training_set = ratings.copy()  # Make a copy of the original data we can alter as our training set.\n",
    "    nonzero_inds = training_set.nonzero()  # Find the indices in the ratings data where an interaction exists\n",
    "    nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1]))  # Zip these pairs together of user,item index into list\n",
    "    random.seed(0)  # Set the random seed to zero for reproducibility\n",
    "    num_samples = int(\n",
    "        np.ceil(pct_test * len(nonzero_pairs)))  # Round the number of samples needed to the nearest integer\n",
    "    samples = random.sample(nonzero_pairs, num_samples)  # Sample a random number of user-item pairs without replacement\n",
    "    user_inds = [index[0] for index in samples]  # Get the user row indices\n",
    "    item_inds = [index[1] for index in samples]  # Get the item column indices\n",
    "    training_set[user_inds, item_inds] = 0  # Assign all of the randomly chosen user-item pairs to zero\n",
    "    training_set.eliminate_zeros()  # Get rid of zeros in sparse array storage after update to save space\n",
    "\n",
    "    # The original return statement\n",
    "    if not return_samples:\n",
    "        return training_set, test_set, list(set(user_inds))  # Output the unique list of user rows that were altered\n",
    "\n",
    "    # New return statement, that gives the indices that were altered\n",
    "    return training_set, test_set, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "micro-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_score(predictions, test):\n",
    "    '''\n",
    "    This simple function will output the area under the curve using sklearn's metrics.\n",
    "\n",
    "    parameters:\n",
    "\n",
    "    - predictions: your prediction output\n",
    "\n",
    "    - test: the actual target result you are comparing to\n",
    "\n",
    "    returns:\n",
    "\n",
    "    - AUC (area under the Receiver Operating Characterisic curve)\n",
    "    '''\n",
    "    # shuffle list of predictions (shuffle function)\n",
    "    # shuffling dissassociates link to artist - > roc of .5 (random)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test, predictions)\n",
    "    return metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dangerous-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(pred, actual, top_n=1000):\n",
    "    sort_idx = np.argsort(pred)\n",
    "    top10 = actual[sort_idx[0:top_n]]\n",
    "    hit_rate = np.sum(top10 > 0) #/ top_n\n",
    "    return hit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "german-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(training_set, altered_users, predictions, test_set):\n",
    "    '''\n",
    "    This function will calculate the mean AUC by user for any user that had their user-item matrix altered.\n",
    "\n",
    "    parameters:\n",
    "\n",
    "    training_set - The training set resulting from make_train, where a certain percentage of the original\n",
    "    user/item interactions are reset to zero to hide them from the model\n",
    "\n",
    "    predictions - The matrix of your predicted ratings for each user/item pair as output from the implicit MF.\n",
    "    These should be stored in a list, with user vectors as item zero and item vectors as item one.\n",
    "\n",
    "    altered_users - The indices of the users where at least one user/item pair was altered from make_train function\n",
    "\n",
    "    test_set - The test set constucted earlier from make_train function\n",
    "\n",
    "    returns:\n",
    "\n",
    "    The mean AUC (area under the Receiver Operator Characteristic curve) of the test set only on user-item interactions\n",
    "    there were originally zero to test ranking ability in addition to the most popular items as a benchmark.\n",
    "    '''\n",
    "\n",
    "    store_auc = []  # An empty list to store the AUC for each user that had an item removed from the training set\n",
    "    popularity_auc = []  # To store popular AUC scores\n",
    "    random_auc = []\n",
    "    store_hit_rate = []\n",
    "    popularity_hit_rate = []\n",
    "    random_hit_rate = []\n",
    "    pop_items = np.array(test_set.sum(axis=0)).reshape(-1)  # Get sum of item iteractions to find most popular\n",
    "    item_vecs = predictions[1]\n",
    "    for user in altered_users:  # Iterate through each user that had an item altered\n",
    "        training_row = training_set[user, :].toarray().reshape(-1)  # Get the training set row\n",
    "        zero_inds = np.where(training_row == 0)  # Find where the interaction had not yet occurred\n",
    "        # Get the predicted values based on our user/item vectors\n",
    "        user_vec = predictions[0][user, :]\n",
    "        pred = user_vec.dot(item_vecs).toarray()[0, zero_inds].reshape(-1)\n",
    "        # Get only the items that were originally zero\n",
    "        # Select all ratings from the MF prediction for this user that originally had no iteraction\n",
    "        actual = test_set[user, :].toarray()[0, zero_inds].reshape(-1)\n",
    "        # Select the binarized yes/no interaction pairs from the original full data\n",
    "        # that align with the same pairs in training\n",
    "        pop = pop_items[zero_inds]  # Get the item popularity for our chosen items\n",
    "        \n",
    "        # pred\n",
    "        curr_auc_score = auc_score(pred, actual)\n",
    "        store_auc.append(curr_auc_score)\n",
    "\n",
    "        curr_hit_rate = hit_rate(pred, actual)\n",
    "        store_hit_rate.append(curr_hit_rate)\n",
    "\n",
    "        # pop\n",
    "        curr_pop_score = auc_score(pop, actual)\n",
    "        popularity_auc.append(curr_pop_score)\n",
    "\n",
    "        curr_pop_hit_rate = hit_rate(pop, actual)\n",
    "        popularity_hit_rate.append(curr_pop_hit_rate)\n",
    "\n",
    "        # random\n",
    "        rnd = np.copy(pred)\n",
    "        np.random.shuffle(rnd)\n",
    "        curr_random_score = auc_score(rnd, actual)\n",
    "        random_auc.append(curr_random_score)\n",
    "\n",
    "        curr_random_hit_rate = hit_rate(rnd, actual)\n",
    "        random_hit_rate.append(curr_random_hit_rate)\n",
    "\n",
    "\n",
    "        # print(user, \"\\t\", curr_auc_score , \"\\t\", curr_pop_score)\n",
    "    # End users iteration\n",
    "\n",
    "    return (float('%.3f' % np.mean(store_auc)),\n",
    "            float('%.3f' % np.mean(popularity_auc)),\n",
    "            float('%.3f' % np.mean(random_auc)),\n",
    "            float('%.3f' % np.mean(store_hit_rate)),\n",
    "            float('%.3f' % np.mean(popularity_hit_rate)),\n",
    "            float('%.3f' % np.mean(random_hit_rate))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "gorgeous-measurement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09:17:43-260221]  This method is deprecated. Please use the AlternatingLeastSquares class instead\n",
      "[09:17:43-260221]  GPU training requires factor size to be a multiple of 32. Increasing factors from 50 to 64.\n",
      "[09:17:43-260221]  OpenBLAS detected. Its highly recommend to set the environment variable 'export OPENBLAS_NUM_THREADS=1' to disable its internal multithreading\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71df5f95f2a4c83bd90cc1c0c3445af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_auc\t0.887\n",
      "popularity_auc\t0.883\n",
      "random_auc\t0.501\n",
      "mean_hit_rate\t0.133\n",
      "popular_hit_rate\t0.068\n",
      "random_hit_rate\t0.343\n"
     ]
    }
   ],
   "source": [
    "# hyper parameters (run Roc_testing.py to get better numbers!)\n",
    "alpha = 16\n",
    "factors = 50\n",
    "regularization = 0.01\n",
    "\n",
    "# train test split\n",
    "u_to_a_train, u_to_a_test, altered_users = make_train(artist_user_matrix.T.tocsr(), pct_test=0.2)\n",
    "\n",
    "# split original matrix into user matrix and artist matrix through ALS\n",
    "user_vecs, artists_vecs = implicit.alternating_least_squares(\n",
    "    (u_to_a_train * alpha).astype('double'),\n",
    "    factors=50,\n",
    "    regularization=regularization,\n",
    "    iterations=50)  # use_GPU=True\n",
    "\n",
    "# evaluate\n",
    "mean_auc, popularity_auc, random_auc, mean_hit_rate, popularity_hit_rate, random_hit_rate = \\\n",
    "    evaluate(u_to_a_train, altered_users, [sparse.csr_matrix(user_vecs), sparse.csr_matrix(artists_vecs.T)],\n",
    "                   u_to_a_test)\n",
    "\n",
    "# write outcomes\n",
    "print(\"mean_auc\\t\" + str(mean_auc))\n",
    "print(\"popularity_auc\\t\" + str(popularity_auc))\n",
    "print(\"random_auc\\t\" + str(random_auc))\n",
    "\n",
    "print(\"mean_hit_rate\\t\" + str(mean_hit_rate))\n",
    "print(\"popular_hit_rate\\t\" + str(popularity_hit_rate))\n",
    "print(\"random_hit_rate\\t\" + str(random_hit_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
